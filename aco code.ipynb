{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c74c115-ef0a-4a5b-b6e4-44004c9abf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05b380a5-ecba-4562-9a03-647a2b18c6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding, SpatialDropout1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from gensim.models import Word2Vec\n",
    "from deap import base, creator, tools, algorithms\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cda409c-92b9-4e6f-8330-fea07c6b7144",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\OMEN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\OMEN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "baa1f797-31e2-45da-89b2-6efd37f67b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    column_names = [\"sentiment\", \"id\", \"date\", \"query\", \"username\", \"text\"]\n",
    "    dataset = pd.read_csv(\"C:/Users/OMEN/Documents/sem6/otaiml/training.1600000.processed.noemoticon.csv\",\n",
    "                          encoding='latin-1', names=column_names, header=None)\n",
    "    dataset = dataset[[\"sentiment\", \"text\"]]\n",
    "    dataset[\"sentiment\"] = dataset[\"sentiment\"].replace({0: 0, 4: 1})  # Convert 4 (positive) to 1\n",
    "    dataset.dropna(inplace=True)\n",
    "    dataset.drop_duplicates(inplace=True)\n",
    "    dataset = dataset[dataset[\"text\"].str.split().str.len() >= 3]  # Remove very short tweets\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d8b4142-b97a-491e-9a96-661cd0f6fe5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'[^a-zA-Z]', ' ', text)  # Keep only letters\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1aebb567-2f76-41bf-8ee0-26ab59b08505",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(dataset):\n",
    "    dataset['text'] = dataset['text'].apply(clean_text)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1651087d-19be-4c5b-978d-3bb23fc2b0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_word2vec(sentences):\n",
    "    model = Word2Vec(sentences, vector_size=100, window=5, min_count=2, workers=4)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50b94c59-29e2-4033-9602-7bc0cd50d74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aco_feature_selection(X, y):\n",
    "    def evaluate(individual):\n",
    "        selected_features = [index for index, value in enumerate(individual) if value == 1]\n",
    "        if len(selected_features) == 0:\n",
    "            return (0,)  # Avoid division by zero\n",
    "        X_selected = X[:, selected_features]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        model = tf.keras.Sequential([\n",
    "            Dense(10, activation='relu', input_shape=(X_selected.shape[1],)),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        model.fit(X_train, y_train, epochs=5, batch_size=64, verbose=0)\n",
    "        \n",
    "        _, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "        return (accuracy,)\n",
    "    if \"FitnessMax\" not in creator.__dict__:\n",
    "        creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
    "    if \"Individual\" not in creator.__dict__:\n",
    "        creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
    "\n",
    "    toolbox = base.Toolbox()\n",
    "    toolbox.register(\"attr_bool\", random.randint, 0, 1)\n",
    "    toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_bool, n=X.shape[1])\n",
    "    toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "    toolbox.register(\"mate\", tools.cxTwoPoint)\n",
    "    toolbox.register(\"mutate\", tools.mutFlipBit, indpb=0.05)\n",
    "    toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
    "    toolbox.register(\"evaluate\", evaluate)\n",
    "\n",
    "    population = toolbox.population(n=20)\n",
    "    algorithms.eaMuPlusLambda(population, toolbox, mu=20, lambda_=40, cxpb=0.5, mutpb=0.2, ngen=10, verbose=True)\n",
    "    \n",
    "    best_individual = tools.selBest(population, 1)[0]\n",
    "    return [index for index, value in enumerate(best_individual) if value == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "560051c9-60e9-4d4c-a535-23776ba92e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lstm(X_train, X_test, y_train, y_test, vocab_size, max_length):\n",
    "    model = Sequential([\n",
    "        Embedding(vocab_size, 100, input_length=max_length),\n",
    "        SpatialDropout1D(0.2),\n",
    "        LSTM(100, dropout=0.2, recurrent_dropout=0.2),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.fit(X_train, y_train, epochs=5, batch_size=64, validation_data=(X_test, y_test), verbose=2)\n",
    "\n",
    "    # Predict and evaluate performance\n",
    "    y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "815fc7bb-7977-4b1c-8239-0f5faa16732b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_data()\n",
    "dataset = preprocess_data(dataset)\n",
    "\n",
    "# Tokenize text for Word2Vec\n",
    "tokenized_texts = [nltk.word_tokenize(text) for text in dataset['text']]\n",
    "\n",
    "# Train Word2Vec model\n",
    "word2vec_model = train_word2vec(tokenized_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41cc6982-2b56-405c-bf55-09ed607b8996",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([\n",
    "    np.mean([word2vec_model.wv[word] for word in words if word in word2vec_model.wv] or [np.zeros(100)], axis=0) \n",
    "    for words in tokenized_texts\n",
    "])\n",
    "\n",
    "y = np.array(dataset['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eeaa388e-7f72-4235-b6b9-3bb898a741f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\OMEN\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen\tnevals\n",
      "0  \t20    \n",
      "1  \t30    \n",
      "2  \t26    \n",
      "3  \t31    \n",
      "4  \t26    \n",
      "5  \t24    \n",
      "6  \t29    \n",
      "7  \t26    \n",
      "8  \t26    \n",
      "9  \t29    \n",
      "10 \t27    \n"
     ]
    }
   ],
   "source": [
    "selected_features = aco_feature_selection(X, y)\n",
    "X_selected = X[:, selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d118cf3-3e66-49b4-9bf1-076a023a5544",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79a100d0-ba2d-4f7d-a6a5-68264fb2a385",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\OMEN\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "19462/19462 - 2238s - 115ms/step - accuracy: 0.5792 - loss: 0.6704 - val_accuracy: 0.6049 - val_loss: 0.6563\n",
      "Epoch 2/5\n",
      "19462/19462 - 2214s - 114ms/step - accuracy: 0.6064 - loss: 0.6539 - val_accuracy: 0.6144 - val_loss: 0.6460\n",
      "Epoch 3/5\n",
      "19462/19462 - 2256s - 116ms/step - accuracy: 0.6135 - loss: 0.6468 - val_accuracy: 0.6202 - val_loss: 0.6400\n",
      "Epoch 4/5\n",
      "19462/19462 - 2259s - 116ms/step - accuracy: 0.6181 - loss: 0.6427 - val_accuracy: 0.6267 - val_loss: 0.6358\n",
      "Epoch 5/5\n",
      "19462/19462 - 2250s - 116ms/step - accuracy: 0.6218 - loss: 0.6393 - val_accuracy: 0.6297 - val_loss: 0.6325\n",
      "\u001b[1m9731/9731\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 13ms/step\n",
      "Accuracy: 0.6297\n",
      "Precision: 0.6242\n",
      "Recall: 0.6544\n",
      "F1 Score: 0.6389\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Sequential name=sequential_294, built=True>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_lstm(X_train, X_test, y_train, y_test, vocab_size=len(word2vec_model.wv), max_length=X_selected.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320da439-dee1-4153-bb3d-5175f7cfa44c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
